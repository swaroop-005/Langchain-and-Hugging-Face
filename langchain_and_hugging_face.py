# -*- coding: utf-8 -*-
"""Langchain and Hugging Face.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P7SV36NHWaefc1DAR_nvvo91--u48B7U

https://youtu.be/bFB4zqkcatU?si=qUBwZvAdrIEKQpqh
"""

!pip install langchain-huggingface

!pip install huggingface_hub
!pip install transformers
!pip install accelerate
!pip install bitsandbytes
!pip install langchain

##Environment secret keys

from google.colab import userdata
sec_key = userdata.get('HF_TOKEN')
print(sec_key)

from langchain_huggingface import HuggingFaceEndpoint

from google.colab import userdata
sec_key = userdata.get('HUGGINGFACEHUB')
print(sec_key)

import os
os.environ["HUGGINGFACEHUB"] = sec_key

repo_id = "mistralai/Mistral-7B-Instruct-v0.3"
llm = HuggingFaceEndpoint(repo_id=repo_id,max_length=512,temperature=0.7,token=sec_key)

llm.invoke("Concepts of Data science")

from langchain import PromptTemplate, LLMChain

question="Who won the latest T20 Cricket world cup?"
template="""Question: {question}
Answer: Let's think step by step."""
prompt = PromptTemplate(template=template, input_variables=["question"])
# print(prompt)
llm_chain = LLMChain(prompt=prompt, llm=llm)
print(llm_chain.invoke(question))

"""HuggingFace Pipeline"""

from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_id="gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

pipe=pipeline("text-generation",model=model,tokenizer=tokenizer,max_new_tokens=100)
hf=HuggingFacePipeline(pipeline=pipe)

hf

hf.invoke("How effective is automation testing in future")

## Use HuggingFace pipelines with GPU

gpu_llm = HuggingFacePipeline.from_model_id(
    model_id="gpt2",
    task="text-generation",
    device_map="auto",  #replace with device=0 or -1 to use the accelerate library.
    pipeline_kwargs={"max_new_tokens": 500},
    )

from langchain import PromptTemplate, LLMChain

question="Who won the latest T20 Cricket world cup?"
template="""Question: {question}
Answer: Let's think step by step."""
prompt = PromptTemplate(template=template, input_variables=["question"])

chain=prompt|gpu_llm

question="How effective is automation testing in future"
chain.invoke({"question":question})